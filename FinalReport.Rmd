---
output:
  html_document:
    theme: flatly
    toc: TRUE
    toc_float: TRUE
---

# Predicting the Unpredictable: Should You Really Be Worried about the Big One?

BY KATJA MCKIERNAN AND KATYA KELLY
\
DECEMBER 17 2017

\

Earthquakes are one of the most unpredictable natural disasters that occur in the western United States. As statisticians who also study geology and geography, we decided to tackle the challenge of making predictions about earthquakes. We're both from cities that have been tormented by a variety of media sources warning of the impending "Big One" (Los Angeles, California and Tacoma, Washington). Just take a peek at this 2015 article heading from *The New Yorker*:

\

![](/Users/Katya/Documents/Bayesian/BayesCapstone/bigOneNewYorker.jpg)

\

Scary, right? Pretty much the entire west coast was on high alert for a couple years, holding city-wide earthquake drills and demanding everyone take the time to nail their bookshelves to the walls. While the hype has since died down a little, we wanted to dive deeper into earthquake predictions. Although the Big One was rumored to possibly exceed a magnitude of 9.0 on the Richter Scale, we defined quakes of magnitude 7+ to be "big" for the sake of having enough data. The first question you might have is: how likely are we to see an earthquake this big in the next year? What about in the next five? We hope to approach an answer to these questions using Bayesian statistical modeling and regression.

# The Data
We collected all of our data from the United States Geological Survey (USGS) website. For predicting magnitude, we gathered 4700 observations of earthquakes above a magnitude of 4.5 since the year 2000. For our yearly rate predictions, we have 88 observations of earthquakes above a magnitude of 7 since 1900.

### Preliminary Exploration
To get acquainted with the data, we created maps to get a sense for where the earthquakes in our data set were occurring. Using the `ggmap` package, we were able to use our latitude and longitude data to plot the earthquakes directly on base maps pulled from Google Maps. Each set of maps shows all earthquakes in the top two maps and only large earthquakes in the bottom two. First, we have earthquakes by their magnitudes:

\

![](/Users/Katya/Documents/Bayesian/BayesCapstone/quakesByMag.jpg)
\
\

Next, we plotted by year:

\

![](/Users/Katya/Documents/Bayesian/BayesCapstone/quakesByYear.jpg)

\

Firstly and importantly, our maps reflect current geological knowledge of plate boundaries, yay! Secondly, you may notice that there are far more smaller/moderate earthquakes than larger ones, with magnitudes 4.5-5 being the most common. This is what we would expect both based on personal knowledge and general earthquake knowledge (looks like those geology and geography minors are useful after all). As far as yearly trends go, it appears that there may be more earthquakes overall in the past 20 years than in the 1950s. That being said, it is possible that smaller earthquakes were not recorded as reliably in earlier years as they are today. Although there aren't any obvious trends, these maps still gave us a few ideas for our analysis. What if we started with a simpler model relating magnitude to geographic region, then moved into a more complex model to estimate the yearly rate of earthquakes? These models would certainly be of significance to those living in earthquake-prone areas.

# Predicting magnitude of the next quake

### The Model

To start off simple, something that may be helpful or interesting to know is how big you can expect the next earthquake to be. Many small earthquakes happen daily in Alaska and on the west coast of the US, where we will focus our analysis. For this reason, we chose to make predictions about earthquakes that people are likely to actually feel.

\
Our data set consists of all magnitude 4.5+ earthquakes in North America from 2000-2017. We added a column to the data that specifies the broad region in which each quake occurred: Alaska, Canada, Hawaii, Mexico, or US. Below are some boxplots we used to explore the relationship between region and magnitude, where magnitude is log-transformed to account for expected skew:

\

![](/Users/Katya/Documents/Bayesian/BayesCapstone/magByRegion.jpeg)

\

From these plots, there appear to be small differences in median magnitudes and in variation by region. Okay, we know — there's nothing especially fascinating going on here, but it's worth an exploration. We can construct a model that we believe captures some of the observed trends. If we let $M_i$ represent the magnitude of earthquake $i$ and $X_i$ be the region in which earthquake $i$ occurred, then we can obtain the following model and priors:

\

$$\log(M_i)|\beta_0, \beta_1, \tau_0, \tau_1 \sim N(\beta_0 + \beta_{1i}X_i, (\tau_0 + \tau_{1i}X_i)^{-1})$$
$$\beta_0, \beta_{1i} \sim N(0, 100^2)$$
$$\tau_0 \sim N(10,100^2)$$
$$\tau_{1i} \sim N(0, 100^2)$$

\

According to this model, we assume that log(magnitude) is Normally distributed. We allow each region its own error term to account for the differences in variation we saw in the boxplots. $\beta_0$ refers to our base expected posterior magnitude for Alaska, and $\tau_0$ is the base precision, which is equivalent to the inverse of variance. Since $X_i$ is a categorical variable, we set Alaska as a reference group and estimate $\beta_{1i}$ and $\tau_{1i}$ as differences from the reference. The regions are in alphabetical order so that beta1[1] is Alaska, followed by Canada, Hawaii, Mexico, and the US (beta1[2], beta1[3], beta1[4], and beta1[5], respectively). Our prior understanding of the parameters $\beta_0, \beta_{i1}, \tau_0$ and $\tau_{1i}$ is vague — we assign an uninformative mean and arbitrarily large variance to each.

### Analysis & results 

Now that we have a model set up, we can start getting into the meat of the prediction process: estimating the parameters that we can use to approximate the posterior model and make our prediction distribution! We begin by using the rjags package to fit our model. We want to run enough simulations of sample posteriors that the posterior mean values of the parameters stabilize. After a few trials, we use a running mean plot function from the `MacBayes` package to determine that 50,000 iterations of the algorithm would suffice. Below is a sample running mean plot for the posterior value of $\beta_0$:

\

![](/Users/Katya/Documents/Bayesian/BayesCapstone/runningMeanEx1.jpeg)

\

Once we are comfortable with our simulations, we can take a look at the posterior distributions of our parameters. First, we see the plots for $\beta_0$ and each $\beta_{1i}$:

\


![](/Users/Katya/Documents/Bayesian/BayesCapstone/postMag1.jpeg) ![](/Users/Katya/Documents/Bayesian/BayesCapstone/postMag2.jpeg)

\

The $\beta_0$ distribution tells us we can expect a base magnitude (in Alaska) of about $e^{1.585}$, which is about 4.88 (our model predicts log(magnitude), remember). Each of the $\beta_{1i}$ plots represents the change in magnitude prediction from Alaska for a given region. Apart from the reference, each $\beta_{1i}$ plot is centered somewhere slightly above 0. This indicates that all regions are expected to have higher magnitude earthquakes than Alaska, on average. This does indeed match the trend we saw in the boxplots, where Alaska's median expected magnitude was the lowest. Depending on how much you know about Alaska, this may be surprising — Alaska has experienced some of the largest earthquakes on record. But Alaska also experiences some of the most *frequent* earthquakes, many of which are not so large. Thus, it's likely that these results are reflective of a relatively small percentage of large earthquakes compared to moderate ones.

\

Next, we see the plots for the error parameters, $\tau_0$ and $\tau_{1i}$:

![](/Users/Katya/Documents/Bayesian/BayesCapstone/postMag3.jpeg) ![](/Users/Katya/Documents/Bayesian/BayesCapstone/postMag4.jpeg) 

\

As with the $\beta_{1i}$ parameters, tau1[1] is set as the reference and represents Alaska. The rest of the $\tau_{1i}$ parameters are centered at negative values, telling us that all other regions tend to have higher precision (lower variance) than Alaska. If you revisit the boxplots from before, you can see that they are consistent with this finding.

\

Okay, enough about the posterior distributions already — let's make some predictions! We want to know how big we can expect the next notable earthquake might be in the continental US (our data), since that's where both of us live. We can store our posterior samples in a data frame and build a vector of predictions based on our region of interest. We assume that the log of the next US quake's magnitude is also Normally distributed as in our original model. Notice that the $\beta_{1i}$ and $\tau_{1i}$ parameters have been adjusted to reflect that we are making predictions for the US, the fifth region:

$$\log(M_{\text{(next US)}}|\beta_0, \beta_{1[5]}, \tau_0, \tau_{1[5]} \sim N(\beta_0 + \beta_{1[5]}\text{US}, (\tau_0 + \tau_{1[5]}\text{US})^{-1})$$

We use the `rnorm` function to take 50,000 random draws from the Normal distribution with the means and variances specified by our posterior simulations. Then we can plot a simple histogram of the predictions:

\

![](/Users/Katya/Documents/Bayesian/BayesCapstone/magPredUS.jpeg)

\

This distribution probably doesn't look too surprising, but we can calculate a couple of statistics we are interested in (see appendix for code). For example, the expected value of the next earthquake's magnitude is about 4.96, and a middle 95% credible interval is [4.2, 5.9]. Only 1 of the 50,000 predictions made was of magnitude 7+. So, based on trends in magnitude since 2000, we rarely expect the next continental US earthquake to be very large. Maybe this makes you feel better, but maybe you're thinking "Wait, this data only spans a measly 18 years where there happened to be no earthquakes above magnitude 7 in this region!" If you're feeling that second sentiment, then 1) congratulations, you're probably more aware than the average person of earthquakes in the continental US, and 2) look no further than the very next section of this blog, where we will look at 88 large earthquakes in North America dating back to 1900 and up to 2016. That's a whole 117 years for you!


# Predicting yearly rate of large quakes

Note to Alicia: we intend to follow the same format as the "Predicting magnitude" section and subsections, but didn't have time to write this one before the draft deadline

### The Model

(Still in progress) Another important study to conduct is figuring out the yearly rate of large earthquakes (above 7 magnitude). To do this we created a poisson model where the rate lambda is based on the longitude range and year of interest. We then got a yearly rate of earthquakes for each longtitude range. 

### Analysis & results


# Limitations and next steps

Every study has its limitations and hopes for the future. For this study, we first want to acknowledge the limitations of our data sets. As you already have noticed and maybe taken personally, the data we used for predicting magnitude only spanned 18 years. Because so many moderately sized earthquakes happen, the data sets become large quickly, and our average-at-best laptops can only handle so much. Addtionally, the methods we used to obtain the data were susceptible to human error, given that we drew focus areas on a map provided by USGS to specify which earthquakes we wanted.

\

Unlike with our magnitude model, our rate model did not fare so well with stabilized posterior parameter values. We're college students — we don't have days to sit around waiting for millions of simulations to run for more complicated models, so we'll take 2,000,000 and call it good.

\

As such students with limited time, we didn't get to explore everything we wanted to. First, we'd like to consider other available variables to predict magnitude, such as earthquake depth. Additionally, there was a suspicious spike in the number of large earthquakes in 1957 (8 of them!) that we would like to investigate. We could break down the regions to be more specific, or expand our focus to other parts of the world so we don't seem so selfish. Another interesting aspect to explore is the relationship between large earthquakes and their fore- and aftershocks, which we could use to improve our prediction models.

# Resources
Packages: `MacBayes`, `dplyr`, `ggplot2`, `lubridate`, `rjags`, `ggmap`, `stringr`, `gridExtra`

\

All data was sourced from the United States Geological Survey (USGS) website.

\

Special thanks to inspiration from articles such as the one at the beginning of this post from *The New Yorker*.

# Appendix - code and supplementary material
Still cleaning code